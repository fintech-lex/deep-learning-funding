{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fintech-lex/deep-learning-funding/blob/main/Deep_Learning_Venture_Fund.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQcB8-H9QIz0"
   },
   "source": [
    "# Venture Funding with Deep Learning\n",
    "\n",
    "### Data Preparation to use on a Neural Network Model\n",
    "\n",
    "Using Pandas and scikit-learn’s `StandardScaler()`, dataset will be used to compile and evaluate the neural network model later.\n",
    "\n",
    "### Compiling and Evaluating a Binary Classification Model Using a Neural Network\n",
    "\n",
    "Leveraging TensorFlow to design a binary classification deep neural network model. This model should use the dataset’s features to predict whether an company funded startup will be successful based on the features in the dataset. The number of inputs determine the number of layers that the model will contain or the number of neurons on each layer. Then, compile and fit your model. Finally, evaluate your binary classification model to calculate the model’s loss and accuracy.\n",
    "\n",
    "### Optimization of the Neural Network Model\n",
    "\n",
    "Using TensorFlow and Keras, we can optimize the model to improve it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BjimzwRvQIz6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOH2XTDrQIz7"
   },
   "source": [
    "---\n",
    "\n",
    "## Prepare the data to be used on a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_BfjJVqQYFl"
   },
   "outputs": [],
   "source": [
    "# Import applicants_data.csv to Google Colab\n",
    "from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "VlDXCDaqQIz8",
    "outputId": "eb4dd5d1-23f8-4d0a-a2bc-4d46b7c4dcb8"
   },
   "outputs": [],
   "source": [
    "# Read the applicants_data.csv file from the Resources folder into a Pandas DataFrame\n",
    "applicant_data_df = pd.read_csv(\"applicants_data.csv\")\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J62Ed8N_QIz9",
    "outputId": "ec30a229-6fa7-41fd-bb52-c17f1d5a0322"
   },
   "outputs": [],
   "source": [
    "# Review the data types associated with the columns\n",
    "applicant_data_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fm65YbyQIz9"
   },
   "source": [
    "### Drop the “EIN” (Employer Identification Number) and “NAME” columns from the DataFrame, because they are not relevant to the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "xSdMLYgdQIz-",
    "outputId": "fb200506-43e2-4350-b4e1-a8e8cbd6ac3f"
   },
   "outputs": [],
   "source": [
    "# Drop the 'EIN' and 'NAME' columns from the DataFrame\n",
    "applicant_data_df = applicant_data_df.drop(columns=['EIN', 'NAME'])\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhrH2N32QIz-"
   },
   "source": [
    "### Encode the dataset’s categorical variables using `OneHotEncoder` function, then placing the encoded variables into a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMVMUIRdQIz_",
    "outputId": "35322232-c302-4265-c551-678276937a02"
   },
   "outputs": [],
   "source": [
    "# Create a list of categorical variables\n",
    "categorical_variables = list(applicant_data_df.dtypes[applicant_data_df.dtypes == \"object\"].index)\n",
    "\n",
    "# Display the categorical variables list\n",
    "categorical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqlYP86WQIz_"
   },
   "outputs": [],
   "source": [
    "# Create OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENsdes7UQI0A",
    "outputId": "91b42305-ba5b-4b82-d1d4-14a23a3bf532"
   },
   "outputs": [],
   "source": [
    "# Encode the categorcal variables using OneHotEncoder\n",
    "encoded_data = enc.fit_transform(applicant_data_df[categorical_variables])\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_data.shape # Just to show that the encoded data is working and fit/shaped fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "eaAYfbfyQI0A",
    "outputId": "febba1c0-9076-47b3-aee8-8e78e9b3fb05"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data)\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmstIfQJQI0A"
   },
   "source": [
    "### Add original DataFrame’s numerical variables to the DataFrame containing the encoded variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgvgIk5ZGOIY",
    "outputId": "8ec6f1c1-8853-4e50-973f-db6b4b9724ca"
   },
   "outputs": [],
   "source": [
    "#Preparing Data to be used below for Numerical Variables and encoding data accordingly to Concatenate Dataframe into original applicant data csv df\n",
    "numerical_variables = list(applicant_data_df.dtypes[applicant_data_df.dtypes == \"int\"].index)\n",
    "\n",
    "numerical_data = enc.fit_transform(applicant_data_df[numerical_variables])\n",
    "\n",
    "num_var_df = pd.DataFrame(numerical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "vBjRnKbpFrDX",
    "outputId": "ac4419fe-6d4d-42e1-afc3-16b7f629deac"
   },
   "outputs": [],
   "source": [
    "# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame\n",
    "encoded_df = pd.concat([num_var_df, applicant_data_df], axis=1)\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdL7xzJ2QI0B"
   },
   "source": [
    "### Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “IS_SUCCESSFUL”. The remaining columns should define the features dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "wcCAbEC9QI0B",
    "outputId": "b27a9f02-1795-402e-d195-8d5067c2840f"
   },
   "outputs": [],
   "source": [
    "# Define the target set y using the IS_SUCCESSFUL column\n",
    "y = encoded_df['IS_SUCCESSFUL']\n",
    "\n",
    "# Display a sample of y\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "R8Ha9vE0QI0C",
    "outputId": "f3a820ad-32f6-4f38-a4ed-72a5a101d6c0"
   },
   "outputs": [],
   "source": [
    "# Define features set X by selecting all columns but IS_SUCCESSFUL\n",
    "X = encoded_df.drop(columns=encoded_df['IS_SUCCESSFUL'])\n",
    "\n",
    "# Review the features DataFrame\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w55xCgSKQI0C"
   },
   "source": [
    "### Split the features and target sets into training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ng4jwFeQI0C"
   },
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Assign the function a random_state equal to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPqVlFLWQI0D"
   },
   "source": [
    "### Use scikit-learn's `StandardScaler` to scale the features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Srri_3BXQI0D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Convert feature names to string data type\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler(X_train)\n",
    "X_test_scaled = X_scaler(X_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
